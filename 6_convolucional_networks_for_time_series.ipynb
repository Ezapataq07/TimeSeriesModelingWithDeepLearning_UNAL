{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c264bb",
   "metadata": {},
   "source": [
    "## Redes convolucionales para series de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e4b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#leer los datos de S&P500\n",
    "import pandas as pd\n",
    "sp500_data = pd.read_csv('/Users/nataliaacevedo/SeriesTemporalesDeepLearning/datasets_taller/SP500.csv', parse_dates=['Date'], index_col='Date')\n",
    "print(sp500_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162e501",
   "metadata": {},
   "source": [
    "```text\n",
    "            log_yield        price\n",
    "Date                              \n",
    "2005-03-28   0.002438  1174.280029\n",
    "2005-03-29  -0.007625  1165.359985\n",
    "2005-03-30   0.013679  1181.410034\n",
    "2005-03-31  -0.000694  1180.589966\n",
    "2005-04-01  -0.006518  1172.920044\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f97f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calcular el rendimiento logarítmico\n",
    "log_returns = np.log(sp500_data['price'] / sp500_data['price'].shift(1))\n",
    "# Crear un DataFrame con los resultados\n",
    "log_returns = log_returns.to_frame(name=\"log_yield\")\n",
    "log_returns[\"price\"] = sp500_data['price']\n",
    "log_returns = log_returns.dropna()\n",
    "\n",
    "# Save the file\n",
    "log_returns.to_csv(\"/Users/nataliaacevedo/SeriesTemporalesDeepLearning/datasets_taller/SP500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e60506",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>log_yield</th>\n",
    "      <th>price</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>count</th>\n",
    "      <td>4975.000000</td>\n",
    "      <td>4975.000000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>mean</th>\n",
    "      <td>0.000324</td>\n",
    "      <td>2367.603443</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>std</th>\n",
    "      <td>0.012155</td>\n",
    "      <td>1277.862566</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>min</th>\n",
    "      <td>-0.127652</td>\n",
    "      <td>676.530029</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>25%</th>\n",
    "      <td>-0.004071</td>\n",
    "      <td>1319.780029</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>50%</th>\n",
    "      <td>0.000725</td>\n",
    "      <td>1998.979980</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>75%</th>\n",
    "      <td>0.005713</td>\n",
    "      <td>3021.760010</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>max</th>\n",
    "      <td>0.109572</td>\n",
    "      <td>6090.270020</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Crear una gráfica de línea para la serie log_yield\n",
    "fig = px.line(log_returns, x=log_returns.index, y='price', title='Precio del S&P 500')\n",
    "fig.update_layout(xaxis_title='Fecha', yaxis_title='Log Yield')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22367a9e",
   "metadata": {},
   "source": [
    "![](convolucional_files/convolucional_11_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una gráfica de línea para la serie log_yield\n",
    "fig = px.line(log_returns, x=log_returns.index, y='log_yield', title='Retornos Logarítmicos del S&P 500')\n",
    "fig.update_layout(xaxis_title='Fecha', yaxis_title='Log Yield')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bae01f",
   "metadata": {},
   "source": [
    "![](convolucional_files/convolucional_12_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3101bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_returns.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ec814",
   "metadata": {},
   "source": [
    "```text\n",
    "            log_yield        price\n",
    "Date                              \n",
    "2005-03-29  -0.007625  1165.359985\n",
    "2005-03-30   0.013679  1181.410034\n",
    "2005-03-31  -0.000694  1180.589966\n",
    "2005-04-01  -0.006518  1172.920044\n",
    "2005-04-04   0.002724  1176.119995\n",
    "```\n",
    "\n",
    "\n",
    "### Crear las secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeab462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "\n",
    "def create_sequences_with_padding(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    padded_data = np.pad(data, (sequence_length - 1, 0), mode='constant', constant_values=0)  # Agregar ceros al inicio\n",
    "    for i in range(len(data)):  # Iterar sobre todos los datos\n",
    "        seq = padded_data[i:i + sequence_length]  # Crear una secuencia de longitud `sequence_length`\n",
    "        target = data.iloc[i]  # El objetivo sigue siendo el valor original\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "sequence_length = 14  # Longitud de la secuencia\n",
    "train_sequences, train_targets = create_sequences_with_padding(log_returns['log_yield'], sequence_length)\n",
    "\n",
    "print(\"Número de secuencias:\", len(train_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443f3eb",
   "metadata": {},
   "source": [
    "```text\n",
    "Número de secuencias: 4975\n",
    "```\n",
    "\n",
    "\n",
    "Las secuencias cuando no existe el número total de datos para completar\n",
    "muestras de tamaño exacto, se deben complementar. Usar padding.\n",
    "\n",
    "### Escalar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b5756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Escalar las secuencias de entrada\n",
    "scaler_x = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = scaler_x.fit_transform(train_sequences.reshape(-1, train_sequences.shape[-1])).reshape(train_sequences.shape)\n",
    "\n",
    "# Escalar las etiquetas de salida\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "y_train_scaled = scaler_y.fit_transform(train_targets.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"y_train_scaled shape:\", y_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f6e5e",
   "metadata": {},
   "source": [
    "```text\n",
    "X_train_scaled shape: (4975, 14)\n",
    "y_train_scaled shape: (4975,)\n",
    "```\n",
    "\n",
    "\n",
    "### Crear el modelo\n",
    "\n",
    "#### Tasa de aprendizaje autoadaptativa\n",
    "\n",
    "Ayuda al algoritmo ADAM a reducir la tasa cuando no hay mejoría en la\n",
    "pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau # type: ignore\n",
    "\n",
    "# Crear un callback para reducir la tasa de aprendizaje\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Métrica a monitorear\n",
    "    factor=0.5,          # Factor por el cual se reduce la tasa de aprendizaje\n",
    "    patience=5,          # Número de épocas sin mejora antes de reducir\n",
    "    min_lr=1e-6          # Tasa de aprendizaje mínima\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4e9a1",
   "metadata": {},
   "source": [
    "#### Estructura mínimo viable para una Red CNN en series de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb459aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Dropout, BatchNormalization, Flatten# type: ignore\n",
    "from tensorflow.keras.regularizers import l2# type: ignore\n",
    "\n",
    "def build_model(filters, kernel_size, dropout_rate, l2_reg):\n",
    "    model = Sequential([\n",
    "        # Definir explícitamente la capa de entrada\n",
    "        Input(shape=(X_train_scaled.shape[1], 1)),  # Usar sequence_length fijo  # type: ignore\n",
    "        # Capa convolucional 1D con regularización L2\n",
    "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "        BatchNormalization(),  # Normalización por lotes después de la capa convolucional\n",
    "        Dropout(dropout_rate),  # Regularización adicional con Dropout\n",
    "        # Aplanar la salida para la capa densa\n",
    "        Flatten(),  # NECESARIO para conectar Conv1D con Dense\n",
    "        # Capa de salida para regresión\n",
    "        Dense(1, activation='linear', kernel_regularizer=l2(l2_reg))\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5177e88",
   "metadata": {},
   "source": [
    "#### Uso de Random Search para optimización de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92285759",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    \"model__filters\": [16, 32, 64],\n",
    "    \"model__kernel_size\": [2, 3, 4, 5],\n",
    "    \"model__dropout_rate\": [0.1, 0.2, 0.3],\n",
    "    \"model__l2_reg\": [0.001, 0.01, 0.1],  # Intensidad de la regularización L2\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"epochs\": [20, 30, 40]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e88a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "model_wrapper = KerasRegressor(\n",
    "    model=build_model,\n",
    "    verbose=0,\n",
    "    fit__callbacks=[reduce_lr],  # Pasar el callback aquí\n",
    "    fit__validation_split=0.2  # Usar el 20% del conjunto de entrenamiento para validación\n",
    ")\n",
    "\n",
    "# Crear el objeto TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "print(\"Número de divisiones (splits):\", tscv.get_n_splits())\n",
    "\n",
    "# Iterar sobre las divisiones\n",
    "for train_index, test_index in tscv.split(X_train_scaled): # type: ignore\n",
    "    X_train, X_test = X_train_scaled[train_index], X_train_scaled[test_index] # type: ignore\n",
    "    y_train, y_test = y_train_scaled[train_index], y_train_scaled[test_index] # type: ignore\n",
    "\n",
    "    print(\"Tamaño del conjunto de entrenamiento:\", X_train.shape, y_train.shape)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21281d2e",
   "metadata": {},
   "source": [
    "```text\n",
    "Número de divisiones (splits): 5\n",
    "Tamaño del conjunto de entrenamiento: (830, 14) (830,)\n",
    "Tamaño del conjunto de entrenamiento: (1659, 14) (1659,)\n",
    "Tamaño del conjunto de entrenamiento: (2488, 14) (2488,)\n",
    "Tamaño del conjunto de entrenamiento: (3317, 14) (3317,)\n",
    "Tamaño del conjunto de entrenamiento: (4146, 14) (4146,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_wrapper,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,  # Número de combinaciones a probar\n",
    "    cv=tscv,  # Validación cruzada específica para series temporales\n",
    "    verbose=0,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Ajustar la búsqueda aleatoria con los datos de entrenamiento\n",
    "random_search.fit(X_train_scaled, y_train_scaled, verbose=0) # type: ignore\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9604e4c9",
   "metadata": {},
   "source": [
    "```text\n",
    "Mejores hiperparámetros encontrados: {'model__l2_reg': 0.1, 'model__kernel_size': 3, 'model__filters': 64, 'model__dropout_rate': 0.1, 'epochs': 40, 'batch_size': 16}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e91660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Ajustar con el mejor modelo los datos escalado de entrenamiento\n",
    "best_model = random_search.best_estimator_.model_\n",
    "\n",
    "# Entrenar el modelo con el programador de tasas de aprendizaje\n",
    "history = best_model.fit(\n",
    "    X_train_scaled, y_train_scaled, # type: ignore\n",
    "    validation_split=0.2,\n",
    "       epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "# El historial de entrenamiento está en history.history\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=history.history['loss'], mode='lines', name='Training Loss'))\n",
    "fig.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines', name='Validation Loss'))\n",
    "fig.update_layout(title='Training and Validation Loss over Epochs',\n",
    "                  xaxis_title='Epochs',\n",
    "                  yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd063a",
   "metadata": {},
   "source": [
    "![](convolucional_files/convolucional_13_1.png)\n",
    "\n",
    "\n",
    "```text\n",
    "Epoch 1/200\n",
    "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0109 - mae: 0.0062 - val_loss: 0.0067 - val_mae: 0.0021 - learning_rate: 6.2500e-05\n",
    "Epoch 2/200\n",
    "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0107 - mae: 0.0061 - val_loss: 0.0061 - val_mae: 0.0014 - learning_rate: 6.2500e-05\n",
    "Epoch 3/200\n",
    "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0106 - mae: 0.0059 - val_loss: 0.0060 - val_mae: 0.0013 - learning_rate: 6.2500e-05\n",
    "Epoch 4/200\n",
    "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0106 - mae: 0.0060 - val_loss: 0.0067 - val_mae: 0.0021 - learning_rate: 6.2500e-05\n",
    "Epoch 5/200\n",
    "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0108 - mae: 0.0061 - val_loss: 0.0060 - val_mae: 0.0013 - learning_rate: 6.2500e-05\n",
    "Epoch 6/200\n",
    "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0107 - mae: 0.0060 - val_loss: 0.0073 - val_mae: 0.0026 - learning_rate: 6.2500e-05\n",
    "Epoch 198/200\n",
    "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0104 - mae: 0.0058 - val_loss: 0.0057 - val_mae: 0.0011 - learning_rate: 1.0000e-06\n",
    "Epoch 199/200\n",
    "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0104 - mae: 0.0058 - val_loss: 0.0058 - val_mae: 0.0012 - learning_rate: 1.0000e-06\n",
    "Epoch 200/200\n",
    "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0106 - mae: 0.0060 - val_loss: 0.0057 - val_mae: 0.0011 - learning_rate: 1.0000e-06\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Predicciones en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predecir los valores en el conjunto de entrenamiento\n",
    "train_predictions_scaled = best_model.predict(X_train_scaled) # type: ignore\n",
    "\n",
    "# Desescalar las predicciones y los valores reales\n",
    "train_predictions = scaler_y.inverse_transform(train_predictions_scaled.reshape(-1, 1)).flatten() # type: ignore\n",
    "train_actual = scaler_y.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten() # type: ignore\n",
    "\n",
    "# Graficar las predicciones frente a los valores reales\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_actual, label=\"Valores reales\", color=\"blue\")\n",
    "plt.plot(train_predictions, label=\"Predicciones\", color=\"orange\", linestyle=\"--\")\n",
    "plt.title(\"Predicciones vs Valores reales (Conjunto de entrenamiento)\")\n",
    "plt.xlabel(\"Índice\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a1996",
   "metadata": {},
   "source": [
    "```text\n",
    "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "![](convolucional_files/convolucional_22_1.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Predicciones en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir los valores en el conjunto de prueba\n",
    "test_predictions_scaled = best_model.predict(X_train_scaled[test_index])\n",
    "\n",
    "# Desescalar las predicciones y los valores reales\n",
    "test_predictions = scaler_y.inverse_transform(test_predictions_scaled.reshape(-1, 1)).flatten()\n",
    "test_actual = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Graficar las predicciones frente a los valores reales\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_actual, label=\"Valores reales\", color=\"blue\")\n",
    "plt.plot(test_predictions, label=\"Predicciones\", color=\"orange\", linestyle=\"--\")\n",
    "plt.title(\"Predicciones vs Valores reales (Conjunto de prueba)\")\n",
    "plt.xlabel(\"Índice\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa5882",
   "metadata": {},
   "source": [
    "```text\n",
    "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 574us/step\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "![](convolucional_files/convolucional_24_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Calcular MAE y MSE\n",
    "mae = mean_absolute_error(test_actual, test_predictions) # pyright: ignore[reportUndefinedVariable]\n",
    "mse = mean_squared_error(test_actual, test_predictions) # pyright: ignore[reportUndefinedVariable]\n",
    "\n",
    "print(f\"Error absoluto medio (MAE): {mae}\")\n",
    "print(f\"Error cuadrático medio (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fdff3a",
   "metadata": {},
   "source": [
    "```text\n",
    "Error absoluto medio (MAE): 0.00014275193203755972\n",
    "Error cuadrático medio (MSE): 3.7687280885778426e-08\n",
    "```\n",
    "\n",
    "\n",
    "#### Prediccion en el problema completo\n",
    "\n",
    "Predicción del valor del Indice SP500 usando la rentabilidad\n",
    "pronosticada y la media movil del precio en el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb11a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Calcular el precio acumulativo a partir de los rendimientos pronosticados\n",
    "initial_price = log_returns['price'].iloc[train_index[-1]]  # Último precio del conjunto de entrenamiento\n",
    "test_prices = [initial_price]\n",
    "\n",
    "for ret in test_predictions:\n",
    "    next_price = test_prices[-1] * (1 + ret)  # Calcular el siguiente precio\n",
    "    test_prices.append(next_price)\n",
    "\n",
    "# Convertir a un array numpy\n",
    "test_prices = np.array(test_prices[1:])  # Excluir el precio inicial\n",
    "\n",
    "# Aplicar una media móvil\n",
    "window_size = 5  # Tamaño de la ventana para la media móvil\n",
    "smoothed_prices = np.convolve(test_prices, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Crear la gráfica con Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Agregar los precios pronosticados\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(test_prices))),\n",
    "    y=test_prices,\n",
    "    mode='lines',\n",
    "    name='Precios pronosticados',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "# Agregar los precios suavizados (ajustar el índice x)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(window_size - 1, len(test_prices))),\n",
    "    y=smoothed_prices,\n",
    "    mode='lines',\n",
    "    name='Precios suavizados (Media móvil)',\n",
    "    line=dict(color='orange', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "# Configurar el diseño de la gráfica\n",
    "fig.update_layout(\n",
    "    title='Precios pronosticados vs Precios suavizados',\n",
    "    xaxis_title='Índice',\n",
    "    yaxis_title='Precio',\n",
    "    hovermode='x unified',\n",
    "    template='plotly_white',\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d746346",
   "metadata": {},
   "source": [
    "![](convolucional_files/convolucional_25_1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
